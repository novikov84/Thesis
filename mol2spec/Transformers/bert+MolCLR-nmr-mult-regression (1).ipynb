{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6f265e-e433-48e9-a783-950e8b12260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7927e74e-d4d2-4a87-962b-6560442669b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdchem import HybridizationType\n",
    "from rdkit.Chem.rdchem import BondType as BT\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from rdkit.Chem import rdMolTransforms\n",
    "from rdkit.Chem.Draw import rdMolDraw2D, rdDepictor, IPythonConsole\n",
    "from rdkit import rdBase\n",
    "blocker = rdBase.BlockLogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3691c785-5a86-4914-9806-0d84add6bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53265f1-214e-4d58-814f-6f7c7b05f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/nick1899/.netrc\n"
     ]
    }
   ],
   "source": [
    "!python3 -m wandb login eb7b1964fb84cd81de96b2a273ecf2bb6254aeac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d91b4-c4dc-4dba-a04c-e786601aab87",
   "metadata": {},
   "source": [
    "### Upload config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4478096-04a8-4fe1-b1b9-8a8720750818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 32, 'warm_up': 2, 'epochs': 10, 'load_graph_model': 'pretrained_gcn', 'save_every_n_epochs': 1, 'fp16_precision': False, 'init_lr': 0.0005, 'weight_decay': '1e-5', 'gpu': 'cuda:3', 'pretrained_roberta_name': 'molberto_ecfp0_2M', 'roberta_model': {'vocab_size': 30522, 'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1}, 'graph_model_type': 'gin', 'graph_model': {'num_layer': 5, 'emb_dim': 500, 'feat_dim': 768, 'drop_ratio': 0, 'pool': 'mean'}, 'graph_aug': 'node', 'dataset': {'num_workers': 1, 'valid_size': 0.1, 'test_size': 0.1}, 'ntxent_loss': {'temperature': 0.1, 'use_cosine_similarity': True}, 'loss_params': {'alpha': 1.0, 'beta': 1.0, 'gamma': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "config = yaml.load(open(\"config-qsar-regression-exact.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "print(config)\n",
    "\n",
    "num_of_shifts = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4b16779-4235-42fe-9091-531e93496fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 32\n"
     ]
    }
   ],
   "source": [
    "print('batch_size =', config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8caa6fa0-9b19-4665-81b8-b122a0c18ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "print('running on device:', config['gpu'])\n",
    "device = torch.device(config['gpu']) if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe4d265d-3a4d-4ec1-b616-18805240e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_config_file(config, log_dir):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    with open(os.path.join(log_dir, 'config.yml'), 'w') as outfile:\n",
    "        yaml.dump(config, outfile, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d58c5-eb7a-4420-b0db-f743e2167213",
   "metadata": {},
   "source": [
    "### Upload and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d5561e6-2477-4ead-9fae-91cae781bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"datasets/nmr_with_ecfp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0acb3637-c146-45dc-a1d3-d9ef1363d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe = dataframe.drop(columns=['mol_id'])\n",
    "dataframe = dataframe[dataframe['num_of_atoms']<51]\n",
    "dataframe = dataframe.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97ce4a4d-2911-4103-aca3-40843f0745e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataframe = dataframe.drop(columns=['num', 'name'])\n",
    "target = 'arrayed_data' # choosing target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1af996b-c085-40c8-871d-1c13490b3279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ppmdict_real</th>\n",
       "      <th>smiles</th>\n",
       "      <th>arrayed_data</th>\n",
       "      <th>ecfp0</th>\n",
       "      <th>num_of_atoms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174226</td>\n",
       "      <td>{'78.74': [0], '93.93': [5], '126.66': [6, 7],...</td>\n",
       "      <td>C=C=Cc1ccccc1</td>\n",
       "      <td>[78.74, 209.77, 126.86, 128.58, 128.58, 93.93,...</td>\n",
       "      <td>['2246997334', '2245900962', '2246703798', '32...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245444</td>\n",
       "      <td>{'40.0': [12], '45.0': [0, 1], '57.0': [13], '...</td>\n",
       "      <td>CN(C)CCNc1cc(-c2ccc3ccccc3c2)nc2ccccc12</td>\n",
       "      <td>[45.0, 45.0, 126.8, 126.3, 124.3, 130.0, 128.1...</td>\n",
       "      <td>['2246728737', '848128881', '2246728737', '224...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>627</td>\n",
       "      <td>{'86.1': [8], '92.8': [7], '120.6': [12], '122...</td>\n",
       "      <td>C(#Cc1cccnc1)c1ccccc1</td>\n",
       "      <td>[128.9, 128.6, 128.6, 123.2, 131.8, 131.8, 138...</td>\n",
       "      <td>['2245900962', '2245900962', '3217380708', '32...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>179202</td>\n",
       "      <td>{'13.7': [0], '21.8': [1], '31.0': [2], '39.5'...</td>\n",
       "      <td>CCCC/C(=C\\[Se]c1ccccc1)[Se]c1ccccc1</td>\n",
       "      <td>[13.7, 21.8, 31.0, 127.2, 127.3, 129.2, 129.2,...</td>\n",
       "      <td>['2246728737', '2245384272', '2245384272', '22...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12775</td>\n",
       "      <td>{'17.1': [0], '123.7': [16], '128.4': [3, 4], ...</td>\n",
       "      <td>Cc1cc(Br)cc2nc(-c3ccccc3)c(-c3ccccc3)nc12</td>\n",
       "      <td>[17.1, 129.9, 130.3, 128.4, 128.4, 129.1, 129....</td>\n",
       "      <td>['2246728737', '3217380708', '3218693969', '32...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                       ppmdict_real  \\\n",
       "0  174226  {'78.74': [0], '93.93': [5], '126.66': [6, 7],...   \n",
       "1  245444  {'40.0': [12], '45.0': [0, 1], '57.0': [13], '...   \n",
       "2     627  {'86.1': [8], '92.8': [7], '120.6': [12], '122...   \n",
       "3  179202  {'13.7': [0], '21.8': [1], '31.0': [2], '39.5'...   \n",
       "4   12775  {'17.1': [0], '123.7': [16], '128.4': [3, 4], ...   \n",
       "\n",
       "                                      smiles  \\\n",
       "0                              C=C=Cc1ccccc1   \n",
       "1    CN(C)CCNc1cc(-c2ccc3ccccc3c2)nc2ccccc12   \n",
       "2                      C(#Cc1cccnc1)c1ccccc1   \n",
       "3        CCCC/C(=C\\[Se]c1ccccc1)[Se]c1ccccc1   \n",
       "4  Cc1cc(Br)cc2nc(-c3ccccc3)c(-c3ccccc3)nc12   \n",
       "\n",
       "                                        arrayed_data  \\\n",
       "0  [78.74, 209.77, 126.86, 128.58, 128.58, 93.93,...   \n",
       "1  [45.0, 45.0, 126.8, 126.3, 124.3, 130.0, 128.1...   \n",
       "2  [128.9, 128.6, 128.6, 123.2, 131.8, 131.8, 138...   \n",
       "3  [13.7, 21.8, 31.0, 127.2, 127.3, 129.2, 129.2,...   \n",
       "4  [17.1, 129.9, 130.3, 128.4, 128.4, 129.1, 129....   \n",
       "\n",
       "                                               ecfp0  num_of_atoms  \n",
       "0  ['2246997334', '2245900962', '2246703798', '32...             9  \n",
       "1  ['2246728737', '848128881', '2246728737', '224...            26  \n",
       "2  ['2245900962', '2245900962', '3217380708', '32...            14  \n",
       "3  ['2246728737', '2245384272', '2245384272', '22...            20  \n",
       "4  ['2246728737', '3217380708', '3218693969', '32...            24  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cae64520-4a9c-453f-949a-737bbd58bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this because pandas thinks columns with arrays are strings\n",
    "def preprocess_data_dataset(df, column):\n",
    "    for row in tqdm(range(len(df))):\n",
    "        str_ints = eval(df.iloc[row][column])\n",
    "        str_fingerprint = ' '.join(str_ints)\n",
    "        df.at[row, column] = str_fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bca17b71-a4eb-4fdf-b760-0a5c815dda87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 81856.05it/s]\n"
     ]
    }
   ],
   "source": [
    "dropping  = []\n",
    "for i in tqdm(range(len(dataframe['ecfp0']))):\n",
    "    if type(dataframe['ecfp0'].iloc[i]) != str:\n",
    "        dropping.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eef808f-5c97-4e67-b943-010d6e7fa6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.drop(dropping)\n",
    "dataframe = dataframe.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1eb1370-13e9-4505-88da-b216213a873f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 9514.45it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocess_data_dataset(dataframe, 'ecfp0') # preprocess ecfp due to invalid storing of array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a98015d9-7190-4d04-af26-365dcde2824b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 26584.42it/s]\n"
     ]
    }
   ],
   "source": [
    "dataframe['arrayed_data'] = dataframe['arrayed_data'].progress_apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8692181-afd8-42a4-997b-312b986b57a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>ppmdict_real</th>\n",
       "      <th>smiles</th>\n",
       "      <th>arrayed_data</th>\n",
       "      <th>ecfp0</th>\n",
       "      <th>num_of_atoms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>174226</td>\n",
       "      <td>{'78.74': [0], '93.93': [5], '126.66': [6, 7],...</td>\n",
       "      <td>C=C=Cc1ccccc1</td>\n",
       "      <td>[78.74, 209.77, 126.86, 128.58, 128.58, 93.93,...</td>\n",
       "      <td>2246997334 2245900962 2246703798 3217380708 32...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>245444</td>\n",
       "      <td>{'40.0': [12], '45.0': [0, 1], '57.0': [13], '...</td>\n",
       "      <td>CN(C)CCNc1cc(-c2ccc3ccccc3c2)nc2ccccc12</td>\n",
       "      <td>[45.0, 45.0, 126.8, 126.3, 124.3, 130.0, 128.1...</td>\n",
       "      <td>2246728737 848128881 2246728737 2245384272 224...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>627</td>\n",
       "      <td>{'86.1': [8], '92.8': [7], '120.6': [12], '122...</td>\n",
       "      <td>C(#Cc1cccnc1)c1ccccc1</td>\n",
       "      <td>[128.9, 128.6, 128.6, 123.2, 131.8, 131.8, 138...</td>\n",
       "      <td>2245900962 2245900962 3217380708 3218693969 32...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>179202</td>\n",
       "      <td>{'13.7': [0], '21.8': [1], '31.0': [2], '39.5'...</td>\n",
       "      <td>CCCC/C(=C\\[Se]c1ccccc1)[Se]c1ccccc1</td>\n",
       "      <td>[13.7, 21.8, 31.0, 127.2, 127.3, 129.2, 129.2,...</td>\n",
       "      <td>2246728737 2245384272 2245384272 2245384272 22...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12775</td>\n",
       "      <td>{'17.1': [0], '123.7': [16], '128.4': [3, 4], ...</td>\n",
       "      <td>Cc1cc(Br)cc2nc(-c3ccccc3)c(-c3ccccc3)nc12</td>\n",
       "      <td>[17.1, 129.9, 130.3, 128.4, 128.4, 129.1, 129....</td>\n",
       "      <td>2246728737 3217380708 3218693969 3217380708 36...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0   index                                       ppmdict_real  \\\n",
       "0        0  174226  {'78.74': [0], '93.93': [5], '126.66': [6, 7],...   \n",
       "1        1  245444  {'40.0': [12], '45.0': [0, 1], '57.0': [13], '...   \n",
       "2        2     627  {'86.1': [8], '92.8': [7], '120.6': [12], '122...   \n",
       "3        3  179202  {'13.7': [0], '21.8': [1], '31.0': [2], '39.5'...   \n",
       "4        4   12775  {'17.1': [0], '123.7': [16], '128.4': [3, 4], ...   \n",
       "\n",
       "                                      smiles  \\\n",
       "0                              C=C=Cc1ccccc1   \n",
       "1    CN(C)CCNc1cc(-c2ccc3ccccc3c2)nc2ccccc12   \n",
       "2                      C(#Cc1cccnc1)c1ccccc1   \n",
       "3        CCCC/C(=C\\[Se]c1ccccc1)[Se]c1ccccc1   \n",
       "4  Cc1cc(Br)cc2nc(-c3ccccc3)c(-c3ccccc3)nc12   \n",
       "\n",
       "                                        arrayed_data  \\\n",
       "0  [78.74, 209.77, 126.86, 128.58, 128.58, 93.93,...   \n",
       "1  [45.0, 45.0, 126.8, 126.3, 124.3, 130.0, 128.1...   \n",
       "2  [128.9, 128.6, 128.6, 123.2, 131.8, 131.8, 138...   \n",
       "3  [13.7, 21.8, 31.0, 127.2, 127.3, 129.2, 129.2,...   \n",
       "4  [17.1, 129.9, 130.3, 128.4, 128.4, 129.1, 129....   \n",
       "\n",
       "                                               ecfp0  num_of_atoms  \n",
       "0  2246997334 2245900962 2246703798 3217380708 32...             9  \n",
       "1  2246728737 848128881 2246728737 2245384272 224...            26  \n",
       "2  2245900962 2245900962 3217380708 3218693969 32...            14  \n",
       "3  2246728737 2245384272 2245384272 2245384272 22...            20  \n",
       "4  2246728737 3217380708 3218693969 3217380708 36...            24  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f2f0329-fd5b-42b1-9369-075a6b333903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 150177.38it/s]\n"
     ]
    }
   ],
   "source": [
    "dataframe['arrayed_data'] = dataframe['arrayed_data'].progress_apply(lambda x: np.asarray(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1680501-2254-4061-9392-e0c54672f337",
   "metadata": {},
   "source": [
    "### Fix of bad padding protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47823fe7-3c85-4733-b1d1-5d38f06a884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 84572.81it/s]\n"
     ]
    }
   ],
   "source": [
    "dropping  = []\n",
    "for i in tqdm(range(len(dataframe['arrayed_data']))):\n",
    "    if len(dataframe['arrayed_data'].iloc[i]) != num_of_shifts:\n",
    "        dropping.append(i)\n",
    "\n",
    "dataframe = dataframe.drop(dropping)\n",
    "#dataframe = dataframe.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba83513c-7ae0-4080-b6d7-addb8be78aa9",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78e9e393-99a7-47e6-8523-4d5ef88f2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = dataframe[target].to_numpy().mean()\n",
    "std = dataframe[target].to_numpy().std()\n",
    "matrix = np.array(dataframe[target].to_numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "592fa119-9c84-4a42-8ea8-814a2ca03377",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_preds = len(dataframe[target].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32690b81-a775-4271-b367-08fc005a9c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_preds):\n",
    "    matrix[:,i] -= mn[i]\n",
    "    matrix[:,i] /= std[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bc33385-abde-4a79-a6e9-c6ffb6e0b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[target] = matrix.tolist()\n",
    "#dataframe['arrayed_data'] = dataframe['arrayed_data'].progress_apply(lambda x: np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27d13783-2b25-4fe2-bc04-14a02fa9b242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3657146214263286,\n",
       " 2.4528125756053263,\n",
       " 0.523606308827743,\n",
       " 0.43746912497137075,\n",
       " 0.3890613820924653,\n",
       " -0.6801269234931228,\n",
       " 0.30470232017903615,\n",
       " 0.3124663943357695,\n",
       " 0.5621099149371945,\n",
       " -1.6942891576572239,\n",
       " -1.3746231117720253,\n",
       " -1.214154661859889,\n",
       " -1.0528204243330668,\n",
       " -0.9034024672949567,\n",
       " -0.7411973030081994,\n",
       " -0.596013776419713,\n",
       " -0.45686966925848904,\n",
       " -0.34558708607568833,\n",
       " -0.2514298432508667,\n",
       " -0.16883025283670966]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['arrayed_data'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b84b3594-84df-4a90-b0cb-f790bcd256dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 61.0300655 ,  87.2510262 , 103.57351528, 112.85737991,\n",
       "       116.22031659, 117.36626638, 115.75804585, 113.34661572,\n",
       "       106.43558952,  96.37207424,  85.9991048 ,  80.40157205,\n",
       "        72.7894869 ,  63.45159389,  51.06505459,  37.95661572,\n",
       "        24.92104803,  15.36378821,   8.21463974,   3.87098253])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85f52b7b-75f4-4e15-8b2c-41ab856a1ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48.42555769, 49.95040184, 44.47326994, 35.93995368, 31.76795224,\n",
       "       34.45866583, 35.77903228, 42.60741162, 48.84171183, 56.8805353 ,\n",
       "       62.56195176, 66.22020619, 69.13760905, 70.23624152, 68.89535941,\n",
       "       63.6841248 , 54.54739001, 44.45706691, 32.67169733, 22.92825171])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287894eb-5d8b-4d23-ab1c-9d3a63a35b8c",
   "metadata": {},
   "source": [
    "### Create Molecule Dataset\n",
    "##### It will generate torch_geometric.data.Data objects for both bert and GIN/GCN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c61aaf7e-b46e-42f0-8981-67d2006f7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "ATOM_LIST = list(range(1,119))\n",
    "CHIRALITY_LIST = [\n",
    "    Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    Chem.rdchem.ChiralType.CHI_OTHER\n",
    "]\n",
    "BOND_LIST = [\n",
    "    Chem.rdchem.BondType.SINGLE, \n",
    "    Chem.rdchem.BondType.DOUBLE, \n",
    "    Chem.rdchem.BondType.TRIPLE, \n",
    "    Chem.rdchem.BondType.AROMATIC\n",
    "]\n",
    "BONDDIR_LIST = [\n",
    "    Chem.rdchem.BondDir.NONE,\n",
    "    Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "    Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c523406-d8d6-4556-a85b-95d2291af7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5d0bdc0-89c4-494d-a76a-5cc7071cca22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick1899/anaconda3/envs/mol/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, dataset: pd.DataFrame, tokenizer, node_mask_percent=0.15, edge_mask_percent=0.25):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.node_mask_percent = node_mask_percent\n",
    "        self.edge_mask_percent = edge_mask_percent\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.model_max_len = 512\n",
    "\n",
    "        self.dataset['graph'] = self.dataset['smiles'].progress_apply(self.get_graph_from_smiles)\n",
    "        #print(self.dataset['graph'].iloc[0])\n",
    "        self.dataset['graph_copy1'] = self.dataset['graph'].progress_apply(lambda x: self.get_augmented_graph_copy(x[0], x[1], x[2], x[3], x[4]))\n",
    "        self.dataset['graph_copy2'] = self.dataset['graph'].progress_apply(lambda x: self.get_augmented_graph_copy(x[0], x[1], x[2], x[3], x[4]))\n",
    "\n",
    "\n",
    "        self.dataset['tokens'] = self.dataset['ecfp0'].progress_apply(self.tokenize)\n",
    "        self.dataset['mlm'] = self.dataset['tokens'].progress_apply(self.apply_mlm)\n",
    "\n",
    "\n",
    "    def get_graph_from_smiles(self, smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return torch.tensor([[], []], dtype=torch.long), \\\n",
    "                    torch.tensor(np.array([]), dtype=torch.long), \\\n",
    "                    torch.tensor(np.array([]), dtype=torch.long), \\\n",
    "                    0\n",
    "    \n",
    "        N = mol.GetNumAtoms()\n",
    "        M = mol.GetNumBonds()\n",
    "    \n",
    "        type_idx = []\n",
    "        chirality_idx = []\n",
    "        atomic_number = []\n",
    "        \n",
    "        for atom in mol.GetAtoms():\n",
    "            type_idx.append(ATOM_LIST.index(atom.GetAtomicNum()))\n",
    "            chirality_idx.append(CHIRALITY_LIST.index(atom.GetChiralTag()))\n",
    "            atomic_number.append(atom.GetAtomicNum())\n",
    "        \n",
    "        x1 = torch.tensor(type_idx, dtype=torch.long).view(-1,1)\n",
    "        x2 = torch.tensor(chirality_idx, dtype=torch.long).view(-1,1)\n",
    "        node_feat = torch.cat([x1, x2], dim=-1)\n",
    "    \n",
    "        row, col, edge_feat = [], [], []\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            row += [start, end]\n",
    "            col += [end, start]\n",
    "            \n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "    \n",
    "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(edge_feat, dtype=torch.long)\n",
    "        num_nodes = N\n",
    "        num_edges = M\n",
    "        return node_feat, edge_index, edge_attr, num_nodes, num_edges\n",
    "\n",
    "    def get_augmented_graph_copy(self, node_feat, edge_index, edge_attr, N, M):\n",
    "        num_mask_nodes = max([1, math.floor(self.node_mask_percent * N)])\n",
    "        \n",
    "        mask_nodes = random.sample(list(range(N)), num_mask_nodes)\n",
    "\n",
    "        node_feat_new = deepcopy(node_feat)\n",
    "        for atom_idx in mask_nodes:\n",
    "            node_feat_new[atom_idx, :] = torch.tensor([len(ATOM_LIST), 0])\n",
    "        \n",
    "        return Data(x=node_feat_new, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    def tokenize(self, item):\n",
    "        return self.tokenizer(item, truncation=True, max_length=512, padding='max_length')\n",
    "\n",
    "    def mlm(self, tensor):\n",
    "        rand = torch.rand(tensor.shape)\n",
    "        # mask random 15% where token is not 0 <s>, 1 <pad>, or 2 <s/>\n",
    "        mask_arr = (rand < .15) * (tensor != 0) * (tensor != 1) * (tensor != 2)\n",
    "        selection = torch.flatten(mask_arr.nonzero()).tolist()\n",
    "        tensor[selection] = 4\n",
    "        return tensor\n",
    "\n",
    "    def apply_mlm(self, sample):\n",
    "        labels = torch.tensor(sample.input_ids)\n",
    "        attention_mask = torch.tensor(sample.attention_mask)\n",
    "        input_ids = self.mlm(labels.detach().clone())\n",
    "        return Data(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset['mlm'].iloc[index], self.dataset['graph_copy1'].iloc[index], self.dataset['graph_copy2'].iloc[index], self.dataset[target].iloc[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get(self):\n",
    "        pass\n",
    "    def len(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "990d0346-4204-4edd-a947-de80e85a685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 916/916 [00:00<00:00, 2375.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 916/916 [00:00<00:00, 8595.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 916/916 [00:00<00:00, 3192.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 916/916 [00:00<00:00, 6039.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 916/916 [00:00<00:00, 3566.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_bert = 'molberto_ecfp0_2M'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_bert)\n",
    "dataset = MoleculeDataset(dataframe, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef882144-9701-4c8e-b169-0e11113172b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "num_train = len(dataset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "    \n",
    "split_tr = int(np.floor(config['dataset']['valid_size'] * num_train))\n",
    "split_test = int(np.floor(config['dataset']['test_size'] * num_train))\n",
    "    \n",
    "train_idx, valid_idx, test_idx = indices[split_tr + split_test : ], indices[: split_tr], indices[split_tr : split_tr + split_test]\n",
    "    \n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    \n",
    "train_dataloader = DataLoader(\n",
    "    dataset, batch_size=config['batch_size'], sampler=train_sampler,\n",
    "    num_workers=config['dataset']['num_workers'], drop_last=True\n",
    ")\n",
    "    \n",
    "eval_dataloader = DataLoader(\n",
    "    dataset, batch_size=config['batch_size'], sampler=valid_sampler,\n",
    "    num_workers=config['dataset']['num_workers'], drop_last=True\n",
    ")\n",
    "    \n",
    "test_dataloader = DataLoader(\n",
    "    dataset, batch_size=config['batch_size'], sampler=test_sampler,\n",
    "    num_workers=config['dataset']['num_workers'], drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a56144-3102-4a21-b86b-8e851dc97026",
   "metadata": {},
   "source": [
    "### Create Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11cc2bf1-7318-4f29-afdb-8d3b70794bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NTXentLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, device, batch_size, temperature, use_cosine_similarity):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)\n",
    "        self.similarity_function = self._get_similarity_function(use_cosine_similarity)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    def _get_similarity_function(self, use_cosine_similarity):\n",
    "        if use_cosine_similarity:\n",
    "            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
    "            return self._cosine_simililarity\n",
    "        else:\n",
    "            return self._dot_simililarity\n",
    "\n",
    "    def _get_correlated_mask(self):\n",
    "        diag = np.eye(2 * self.batch_size)\n",
    "        l1 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=-self.batch_size)\n",
    "        l2 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=self.batch_size)\n",
    "        mask = torch.from_numpy((diag + l1 + l2))\n",
    "        mask = (1 - mask).type(torch.bool)\n",
    "        return mask.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def _dot_simililarity(x, y):\n",
    "        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n",
    "        # x shape: (N, 1, C)\n",
    "        # y shape: (1, C, 2N)\n",
    "        # v shape: (N, 2N)\n",
    "        return v\n",
    "\n",
    "    def _cosine_simililarity(self, x, y):\n",
    "        # x shape: (N, 1, C)\n",
    "        # y shape: (1, 2N, C)\n",
    "        # v shape: (N, 2N)\n",
    "        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n",
    "        return v\n",
    "\n",
    "    def forward(self, zis, zjs):\n",
    "        representations = torch.cat([zjs, zis], dim=0)\n",
    "\n",
    "        similarity_matrix = self.similarity_function(representations, representations)\n",
    "\n",
    "        # filter out the scores from the positive samples\n",
    "        l_pos = torch.diag(similarity_matrix, self.batch_size)\n",
    "        r_pos = torch.diag(similarity_matrix, -self.batch_size)\n",
    "        positives = torch.cat([l_pos, r_pos]).view(2 * self.batch_size, 1)\n",
    "        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(2 * self.batch_size, -1)\n",
    "\n",
    "        logits = torch.cat((positives, negatives), dim=1)\n",
    "        logits = logits.abs() + 0.0001\n",
    "        logits = torch.log(logits)\n",
    "        logits /= self.temperature\n",
    "        \n",
    "        labels = torch.zeros(2 * self.batch_size).to(self.device).long()\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        return loss / (2 * self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66254640-38dc-4da6-bccf-634174aca108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import RobertaConfig\n",
    "from torch import nn\n",
    "\n",
    "if config['graph_model_type'] == 'gin':\n",
    "    from MolCLR.models.ginnet_old import GINet as GraphModel\n",
    "elif config['graph_model_type'] == 'gcn':\n",
    "    from MolCLR.models.gcn_molclr import GCN as GraphModel\n",
    "else:\n",
    "    raise ValueError('GNN model is not defined in config.')\n",
    "\n",
    "class MolecularBertGraph(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularBertGraph, self).__init__()\n",
    "        self.batch_size = config['batch_size']\n",
    "\n",
    "        roberta_config = roberta_config = RobertaConfig(\n",
    "            vocab_size=30_522,\n",
    "            max_position_embeddings=514,\n",
    "            hidden_size=768,\n",
    "            num_attention_heads=12,\n",
    "            num_hidden_layers=6,\n",
    "            type_vocab_size=1\n",
    "        )\n",
    "        \n",
    "        self.bert = RobertaForMaskedLM(roberta_config).to(device)\n",
    "\n",
    "        self.graph_model = GraphModel(**config['graph_model']).to(device)\n",
    "        # self.graph_model = self._load_graph_pretrained_weights(self.graph_model)\n",
    "\n",
    "        self.out_graph_linear = torch.nn.Linear(2 * config['graph_model']['feat_dim'], \n",
    "                                                768, bias=True)\n",
    "\n",
    "        self.out_graph_projection1 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn1_graph = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_graph_projection2 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn2_graph = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_bert_projection1 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn1_bert = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_bert_projection2 = torch.nn.Linear(768, 768, bias=True)\n",
    "        \n",
    "        self.bn2_bert = nn.BatchNorm1d(768)\n",
    "        \n",
    "        # contrastive loss for MolCLR\n",
    "        self.nt_xent_criterion = NTXentLoss(device, self.batch_size, **config['ntxent_loss'])\n",
    "\n",
    "    def forward(self, bert_batch, graph_batch1, graph_batch2):\n",
    "        bert_output = self.bert(input_ids=bert_batch['input_ids'].view(self.batch_size, -1), \n",
    "                                 attention_mask=bert_batch['attention_mask'].view(self.batch_size, -1),\n",
    "                                 labels=bert_batch['labels'].view(self.batch_size, -1), output_hidden_states=True)\n",
    "        bert_loss = bert_output.loss\n",
    "        bert_emb = bert_output.hidden_states[0][:, 0, :] # take emb for CLS token\n",
    "\n",
    "        graph_loss, hidden_states_1, hidden_states_2 = self.graph_step(graph_batch1, graph_batch2)\n",
    "        \n",
    "        graph_emb = self.out_graph_linear(torch.cat((hidden_states_1, hidden_states_2), dim=-1))\n",
    "\n",
    "        graph_emb_projected1 = self.out_graph_projection1(graph_emb)\n",
    "        \n",
    "        graph_emb_projected_bn1 = self.bn1_graph(graph_emb_projected1)\n",
    "\n",
    "        graph_emb_projected2 = self.out_graph_projection2(torch.nn.functional.relu(graph_emb_projected_bn1))\n",
    "        \n",
    "        graph_emb_projected_bn2 = self.bn2_graph(graph_emb_projected2)\n",
    "        \n",
    "        #bert projections:\n",
    "        bert_emb_projected1 = self.out_bert_projection1(bert_emb)\n",
    "        \n",
    "        bert_emb_projected_bn1 = self.bn1_bert(bert_emb_projected1)\n",
    "\n",
    "        bert_emb_projected2 = self.out_bert_projection2(torch.nn.functional.relu(bert_emb_projected_bn1))\n",
    "        \n",
    "        bert_emb_projected_bn2 = self.bn2_bert(bert_emb_projected2)\n",
    "\n",
    "        # bimodal_loss = ((1 - self.cosine_sim(bert_emb, graph_emb))**2).mean()\n",
    "        bimodal_loss = self.nt_xent_criterion(bert_emb_projected_bn2, graph_emb_projected_bn2)\n",
    "        return bert_loss, graph_loss, bimodal_loss, graph_emb_projected_bn2, bert_emb_projected_bn2\n",
    "\n",
    "    def graph_step(self, xis, xjs):\n",
    "        # get the representations and the projections\n",
    "        ris, zis = self.graph_model(xis)  # [N,C]\n",
    "    \n",
    "        # get the representations and the projections\n",
    "        rjs, zjs = self.graph_model(xjs)  # [N,C]\n",
    "    \n",
    "        # normalize projection feature vectors\n",
    "        zis = torch.nn.functional.normalize(zis, dim=1)\n",
    "        zjs = torch.nn.functional.normalize(zjs, dim=1)\n",
    "\n",
    "        loss = self.nt_xent_criterion(zis, zjs)\n",
    "        return loss, ris, rjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08055762-1a53-49e6-8cd7-71161852abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = MolecularBertGraph().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14c66953-4356-415d-ac4e-e031126c8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b0c73-a897-407e-a8c7-158283f97de4",
   "metadata": {},
   "source": [
    "### Define utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54ef89d4-b7ac-4a86-ab14-67383fd0f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33morlov-aleksei53\u001b[0m (\u001b[33mmoleculary-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nick1899/Transformers-for-Molecules/bert+MolCLR/wandb/run-20241011_201450-kkxuenwn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/kkxuenwn' target=\"_blank\">NMR arrayed_data Multiple-all metrics gin (bert+molclr)</a></strong> to <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/kkxuenwn' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer/runs/kkxuenwn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/moleculary-ai/efcp_transformer/runs/kkxuenwn?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f58d0482d30>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"efcp_transformer\",\n",
    "    name=\"NMR \" + target + \" Multiple-all metrics \" + config['graph_model_type'] + \" (bert+molclr)\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a951a1-7c75-47bd-b275-9693eb22fe95",
   "metadata": {},
   "source": [
    "### Training (with validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5ea2e1e-8d54-4f03-914f-6b6328b5da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dabd36a8-610d-4760-a984-79bf17e5b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "model_checkpoints_folder = os.path.join('ckpts')\n",
    "dir_name = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "log_dir = os.path.join(model_checkpoints_folder, dir_name)\n",
    "_save_config_file(config, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4df3a5c-965c-4a66-bd38-a8158398ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularPropertiesRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularPropertiesRegression, self).__init__()\n",
    "\n",
    "        self.test = MolecularBertGraph().to(device)\n",
    "        self.test.load_state_dict(torch.load('weights_pretr/' + config['graph_model_type'] + '.pth'))\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(768 * 2, 768, bias=True)\n",
    "        self.linear2 = torch.nn.Linear(768, 512, bias=True)\n",
    "        self.bn = nn.BatchNorm1d(512)\n",
    "        self.linear3 = torch.nn.Linear(512, 256, bias=True)\n",
    "        \n",
    "        # Creating output heads: one for each property\n",
    "        self.output_heads = nn.ModuleList([torch.nn.Linear(256, 1) for _ in range(number_of_preds)])\n",
    "\n",
    "    def forward(self, b, g1, g2):\n",
    "        l1, l2, l3, graph_emb, bert_emb = self.test(b, g1, g2)\n",
    "\n",
    "        first_linear_out = self.linear1(torch.cat((graph_emb, bert_emb), dim=-1))\n",
    "        sec_linear_out = self.linear2(torch.nn.functional.leaky_relu(first_linear_out))\n",
    "        batchnormed = self.bn(sec_linear_out)\n",
    "        thd_linear_out = self.linear3(torch.nn.functional.leaky_relu(batchnormed))\n",
    "\n",
    "        # Output for each head\n",
    "        logits = [head(thd_linear_out) for head in self.output_heads]\n",
    "        \n",
    "        return torch.cat(logits, dim=-1)  # Concatenate outputs along the last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca196530-c8af-4581-b213-affedb0109db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MolecularPropertiesRegression().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3826d7c-9d81-4c05-87f3-9bbde4986f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MolecularPropertiesRegression(\n",
       "  (test): MolecularBertGraph(\n",
       "    (bert): RobertaForMaskedLM(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (graph_model): GINet(\n",
       "      (x_embedding1): Embedding(119, 500)\n",
       "      (x_embedding2): Embedding(3, 500)\n",
       "      (gnns): ModuleList(\n",
       "        (0-4): 5 x GINEConv()\n",
       "      )\n",
       "      (batch_norms): ModuleList(\n",
       "        (0-4): 5 x BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (feat_lin): Linear(in_features=500, out_features=768, bias=True)\n",
       "      (out_lin): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=768, out_features=384, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (out_graph_linear): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    (out_graph_projection1): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (bn1_graph): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (out_graph_projection2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (bn2_graph): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (out_bert_projection1): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (bn1_bert): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (out_bert_projection2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (bn2_bert): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (nt_xent_criterion): NTXentLoss(\n",
       "      (softmax): Softmax(dim=-1)\n",
       "      (_cosine_similarity): CosineSimilarity()\n",
       "      (criterion): CrossEntropyLoss()\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (linear2): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (output_heads): ModuleList(\n",
       "    (0-19): 20 x Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0877c631-0ca2-4527-a2d3-009122d28b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = config['epochs']\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), float(config['init_lr']), \n",
    "    weight_decay=eval(config['weight_decay'])\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config['epochs']-config['warm_up'], \n",
    "    eta_min=0, last_epoch=-1\n",
    ")\n",
    "#loss_func = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "368352a9-262c-415b-a981-55a36056b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reform(tens):\n",
    "    stacked_tensor = torch.stack(tens)  # This will give you a shape of (30, 30, 16)\n",
    "    return stacked_tensor.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45f7a9b3-0b5c-4915-a501-57123a462505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "566ab028-6f92-4e7c-98bc-6046c8757cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # Importing functional module\n",
    "\n",
    "class MultiHeadLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        losses = []\n",
    "        \n",
    "        for i in range(predictions.size(1)):  # Iterate over each property\n",
    "            property_predictions = predictions[:, i]\n",
    "            property_targets = targets[:, i]\n",
    "            \n",
    "            # Compute MSE for this property\n",
    "            loss = F.mse_loss(property_predictions, property_targets)\n",
    "            #loss = F.l1_loss(property_predictions, property_targets)\n",
    "            losses.append(loss)\n",
    "\n",
    "        # Return a list of losses for each property\n",
    "        return losses\n",
    "\n",
    "def train_loop():\n",
    "    train_tqdm = tqdm(train_dataloader, unit=\"batch\")\n",
    "    train_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    loss_sum = 0\n",
    "    total_pred_labels = []\n",
    "    total_true_labels = []\n",
    "    model.train()\n",
    "    criterion = MultiHeadLoss()  # Initialize the multi-head loss\n",
    "\n",
    "    for (bert_batch, graph_batch1, graph_batch2, targets) in train_tqdm:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        bert_batch = bert_batch.to(device)\n",
    "        graph_batch1 = graph_batch1.to(device)\n",
    "        graph_batch2 = graph_batch2.to(device)\n",
    "        targets = reform(targets).clone().detach().to(device)\n",
    "\n",
    "        #print(bert_batch)\n",
    "    \n",
    "        with autocast():  # Mixed precision context\n",
    "            pred_labels = model(bert_batch, graph_batch1, graph_batch2)\n",
    "            # Compute the separate losses\n",
    "            losses = criterion(pred_labels, targets)\n",
    "    \n",
    "        # Backward pass for each loss\n",
    "        for loss in losses:\n",
    "            scaler.scale(loss).backward(retain_graph=True)\n",
    "                #loss.backward(retain_graph=True)  # Retain the graph for multiple backward passes\n",
    "    \n",
    "            # After accumulating all gradients, optimize\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        #optimizer.step()  # Update the model parameters\n",
    "    \n",
    "        pred_labels = pred_labels\n",
    "        true_labels = targets\n",
    "        total_pred_labels.append(pred_labels.detach().cpu())\n",
    "        total_true_labels.append(true_labels.detach().cpu())\n",
    "\n",
    "        loss_sum += sum([loss.item() for loss in losses])  # Sum for total loss log\n",
    "    \n",
    "        wandb.log({\"loss/train\": sum([loss.item() for loss in losses])})  # Log the total loss\n",
    "    \n",
    "        train_tqdm.set_postfix(loss=loss_sum / len(total_pred_labels))\n",
    "\n",
    "        del pred_labels\n",
    "        del true_labels\n",
    "        del losses\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "       # optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    return loss_sum / len(train_dataloader), total_pred_labels, total_true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c431e43-da30-40d3-bff0-4e06bfde9db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def train_loop():\\n    train_tqdm = tqdm(train_dataloader, unit=\"batch\")\\n    train_tqdm.set_description(f\\'Epoch {epoch_counter}\\')\\n    loss_sum = 0\\n    total_pred_labels = []\\n    total_true_labels = []\\n    \\n    model.train()\\n    for (bert_batch, graph_batch1, graph_batch2, targets) in train_tqdm:\\n        optimizer.zero_grad()\\n\\n        bert_batch = bert_batch.to(device)\\n        graph_batch1 = graph_batch1.to(device)\\n        graph_batch2 = graph_batch2.to(device)\\n        #print(targets)\\n        #print(reform(targets))\\n        targets = reform(targets).clone().detach().to(device)\\n\\n        pred_labels = model(bert_batch, graph_batch1, graph_batch2)\\n        #loss = loss_func(pred_labels.view(-1), targets.float())\\n        loss = loss_func(pred_labels, targets.float())\\n        loss.backward()\\n\\n\\n        pred_labels = pred_labels.reshape(-1)\\n        true_labels = targets.reshape(-1)\\n        \\n        total_pred_labels.append(pred_labels)\\n        total_true_labels.append(true_labels)\\n        \\n        loss_sum += loss.item()\\n\\n        #wandb.log({“bert_loss/train”:bert_loss, “graph_loss/train”: graph_loss, \"bimodal_loss/train\": bimodal_loss, \"loss/train\": loss})\\n        wandb.log({\"loss/train\": loss})\\n\\n\\n        optimizer.step()\\n        train_tqdm.set_postfix(loss=loss.item())\\n    return loss_sum / len(train_dataloader), total_pred_labels, total_true_labels'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def train_loop():\n",
    "    train_tqdm = tqdm(train_dataloader, unit=\"batch\")\n",
    "    train_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    loss_sum = 0\n",
    "    total_pred_labels = []\n",
    "    total_true_labels = []\n",
    "    \n",
    "    model.train()\n",
    "    for (bert_batch, graph_batch1, graph_batch2, targets) in train_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        bert_batch = bert_batch.to(device)\n",
    "        graph_batch1 = graph_batch1.to(device)\n",
    "        graph_batch2 = graph_batch2.to(device)\n",
    "        #print(targets)\n",
    "        #print(reform(targets))\n",
    "        targets = reform(targets).clone().detach().to(device)\n",
    "\n",
    "        pred_labels = model(bert_batch, graph_batch1, graph_batch2)\n",
    "        #loss = loss_func(pred_labels.view(-1), targets.float())\n",
    "        loss = loss_func(pred_labels, targets.float())\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        pred_labels = pred_labels.reshape(-1)\n",
    "        true_labels = targets.reshape(-1)\n",
    "        \n",
    "        total_pred_labels.append(pred_labels)\n",
    "        total_true_labels.append(true_labels)\n",
    "        \n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        #wandb.log({“bert_loss/train”:bert_loss, “graph_loss/train”: graph_loss, \"bimodal_loss/train\": bimodal_loss, \"loss/train\": loss})\n",
    "        wandb.log({\"loss/train\": loss})\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "        train_tqdm.set_postfix(loss=loss.item())\n",
    "    return loss_sum / len(train_dataloader), total_pred_labels, total_true_labels'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5987f43f-3072-46d2-8b93-7cec0f01ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = MultiHeadLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc579384-660b-4979-80ce-199a40c953e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop():\n",
    "    eval_tqdm = tqdm(eval_dataloader, unit=\"batch\")\n",
    "    eval_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    loss_sum = 0\n",
    "    total_pred_labels = []\n",
    "    total_true_labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    for (bert_batch, graph_batch1, graph_batch2, targets) in eval_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        bert_batch = bert_batch.to(device)\n",
    "        graph_batch1 = graph_batch1.to(device)\n",
    "        graph_batch2 = graph_batch2.to(device)\n",
    "        targets = reform(targets).clone().detach().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_labels = model(bert_batch, graph_batch1, graph_batch2)\n",
    "\n",
    "\n",
    "        losses = loss_func(pred_labels, targets.float())\n",
    "\n",
    "        pred_labels = pred_labels\n",
    "        true_labels = targets\n",
    "        total_pred_labels.append(pred_labels.detach().cpu())\n",
    "        total_true_labels.append(true_labels.detach().cpu())\n",
    "\n",
    "        loss_sum += sum([loss.item() for loss in losses]) \n",
    "\n",
    "        eval_tqdm.set_postfix(loss=loss_sum / len(total_pred_labels))\n",
    "\n",
    "        del pred_labels\n",
    "        del true_labels\n",
    "        del losses\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "    return loss_sum / len(eval_dataloader), total_pred_labels, total_true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "abf8e101-1ffe-4b6d-9f2d-2d44b5d4d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop():\n",
    "    test_tqdm = tqdm(test_dataloader, unit=\"batch\")\n",
    "    test_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    loss_sum = 0\n",
    "    total_pred_labels = []\n",
    "    total_true_labels = []\n",
    "\n",
    "    criterion = MultiHeadLoss()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    for (bert_batch, graph_batch1, graph_batch2, targets) in test_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        bert_batch = bert_batch.to(device)\n",
    "        graph_batch1 = graph_batch1.to(device)\n",
    "        graph_batch2 = graph_batch2.to(device)\n",
    "        targets = reform(targets).clone().detach().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_labels = model(bert_batch, graph_batch1, graph_batch2)\n",
    "\n",
    "        losses = loss_func(pred_labels, targets.float())\n",
    "\n",
    "        pred_labels = pred_labels\n",
    "        true_labels = targets\n",
    "        total_pred_labels.append(pred_labels.detach().cpu())\n",
    "        total_true_labels.append(true_labels.detach().cpu())\n",
    "\n",
    "        loss_sum += sum([loss.item() for loss in losses]) \n",
    "\n",
    "        eval_tqdm.set_postfix(loss=loss_sum / len(total_pred_labels))\n",
    "\n",
    "        del pred_labels\n",
    "        del true_labels\n",
    "        del losses\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "    return loss_sum / len(test_tqdm), total_pred_labels, total_true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc1df98b-7cfb-4841-87a5-36f767b6823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, roc_curve, auc\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6fe5b225-316d-426b-ae91-7bbbdbaf55ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b7c86eb-87b2-4458-ba6e-1d51ddb2346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_norm(tensors_list):\n",
    "    transformed_list = tensors_list#[batch[i, :].clone() for batch in tensors_list for i in range(np.shape(batch)[0])]\n",
    "    for i in range(number_of_preds):\n",
    "        transformed_list[:,i] *= std[i]\n",
    "        transformed_list[:,i] += mn[i]\n",
    "    return transformed_list\n",
    "\n",
    "def inverse(total_true_labels, total_pred_labels):\n",
    "    losses =  loss_func(torch.tensor(invert_norm(total_pred_labels)), torch.tensor(invert_norm(total_true_labels)))\n",
    "    loss_sum = sum([loss.item() for loss in losses])\n",
    "    return float(loss_sum/number_of_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0423f7a4-5757-43b0-8f17-23f1fd58da2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                                                                                        | 0/22 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(input_ids=[16384], attention_mask=[16384], labels=[16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|███████▌                                                                                                                                                             | 1/22 [00:01<00:34,  1.66s/batch, loss=17.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(input_ids=[16384], attention_mask=[16384], labels=[16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   9%|███████████████▏                                                                                                                                                       | 2/22 [00:02<00:23,  1.16s/batch, loss=19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(input_ids=[16384], attention_mask=[16384], labels=[16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  14%|██████████████████████▌                                                                                                                                              | 3/22 [00:03<00:18,  1.01batch/s, loss=18.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(input_ids=[16384], attention_mask=[16384], labels=[16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  18%|██████████████████████████████                                                                                                                                       | 4/22 [00:04<00:18,  1.04s/batch, loss=18.6]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m best_valid_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_counter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch):\n\u001b[0;32m----> 6\u001b[0m     loss, total_pred_labels, total_true_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     total_pred_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(total_pred_labels)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      9\u001b[0m     total_true_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(total_true_labels)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[47], line 33\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     31\u001b[0m criterion \u001b[38;5;241m=\u001b[39m MultiHeadLoss()  \u001b[38;5;66;03m# Initialize the multi-head loss\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (bert_batch, graph_batch1, graph_batch2, targets) \u001b[38;5;129;01min\u001b[39;00m train_tqdm:\n\u001b[1;32m     35\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     37\u001b[0m     bert_batch \u001b[38;5;241m=\u001b[39m bert_batch\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/mol/lib/python3.9/site-packages/tqdm/std.py:1191\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m dt \u001b[38;5;241m=\u001b[39m cur_t \u001b[38;5;241m-\u001b[39m last_print_t\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m mininterval \u001b[38;5;129;01mand\u001b[39;00m cur_t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_start_t:\n\u001b[0;32m-> 1191\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlast_print_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m     last_print_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_n\n\u001b[1;32m   1193\u001b[0m     last_print_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_t\n",
      "File \u001b[0;32m~/anaconda3/envs/mol/lib/python3.9/site-packages/tqdm/std.py:1242\u001b[0m, in \u001b[0;36mtqdm.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dn(dn)\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dt(dt)\n\u001b[0;32m-> 1242\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlock_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlock_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_miniters:\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;66;03m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;66;03m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;66;03m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;66;03m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# at least 5 more iterations.\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval \u001b[38;5;129;01mand\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval:\n",
      "File \u001b[0;32m~/anaconda3/envs/mol/lib/python3.9/site-packages/tqdm/std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m-> 1347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/mol/lib/python3.9/site-packages/tqdm/std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[0;32m-> 1495\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[0;32m~/anaconda3/envs/mol/lib/python3.9/site-packages/tqdm/std.py:459\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[1;32m    458\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[0;32m--> 459\u001b[0m     \u001b[43mfp_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlast_len\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlen_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[0;32m~/anaconda3/envs/mol/lib/python3.9/site-packages/tqdm/std.py:453\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[1;32m    452\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[0;32m--> 453\u001b[0m     \u001b[43mfp_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mol/lib/python3.9/site-packages/tqdm/utils.py:196\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mol/lib/python3.9/site-packages/ipykernel/iostream.py:604\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"trigger actual zmq send\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03msend will happen in the background thread\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mthread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    602\u001b[0m ):\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;66;03m# request flush on the background thread\u001b[39;00m\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpub_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flush\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# wait for flush to actually get through, if we can.\u001b[39;00m\n\u001b[1;32m    606\u001b[0m     evt \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mEvent()\n",
      "File \u001b[0;32m~/anaconda3/envs/mol/lib/python3.9/site-packages/ipykernel/iostream.py:267\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     f()\n",
      "File \u001b[0;32m~/anaconda3/envs/mol/lib/python3.9/site-packages/zmq/sugar/socket.py:701\u001b[0m, in \u001b[0;36mSocket.send\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mFrame(\n\u001b[1;32m    695\u001b[0m             data,\n\u001b[1;32m    696\u001b[0m             track\u001b[38;5;241m=\u001b[39mtrack,\n\u001b[1;32m    697\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    698\u001b[0m             copy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_threshold,\n\u001b[1;32m    699\u001b[0m         )\n\u001b[1;32m    700\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m--> 701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m_zmq.py:1073\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_zmq.py:1121\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_zmq.py:1320\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq._send_copy\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_zmq.py:141\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iter = 0\n",
    "valid_n_iter = 0\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch_counter in range(num_epoch):\n",
    "    loss, total_pred_labels, total_true_labels = train_loop()\n",
    "    \n",
    "    total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "    total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "\n",
    "    wandb.log({\"loss/train-average\": inverse(total_pred_labels, total_true_labels)})\n",
    "\n",
    "    loss, total_pred_labels, total_true_labels = eval_loop()\n",
    "\n",
    "    total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "    total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "\n",
    "    wandb.log({\"loss/eval\": inverse(total_pred_labels, total_true_labels)})\n",
    "\n",
    "    if loss < best_valid_loss:\n",
    "        best_valid_loss = loss\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'model_nmr.pth'))\n",
    "    \n",
    "    if (epoch_counter + 1) % config['save_every_n_epochs'] == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'model_nmr_{}.pth'.format(str(epoch_counter))))\n",
    "\n",
    "    if epoch_counter >= config['warm_up']:\n",
    "                scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da77412-b5e1-4f61-ae28-c1d8fc873e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, total_pred_labels, total_true_labels = test_loop()\n",
    "\n",
    "total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "\n",
    "wandb.log({\"loss/test\": inverse(total_pred_labels, total_true_labels)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ad7b9-f82b-4936-bd4f-26266cbb6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449321b6-da16-4167-adff-8994e62a2194",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9084542-1308-4e91-9445-2a09d0e442ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(total_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df896b-0c45-4f34-a7ba-87386ad96899",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(invert_norm(total_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63479d0b-2bd6-437b-9eaa-03a06dfbf1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
